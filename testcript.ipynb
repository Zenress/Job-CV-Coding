{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export VIVO archive to Data Lake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAS BEEN RUN AND DATA IS SAVED ON DELTA TABLE IN PERSONAL FOLDER\n",
    "\n",
    "Details of User Story: https://jira.dsb.dk/browse/AI1-276\n",
    "\n",
    "The VIVO archive data is on server DSBAZAPP0317.\n",
    "\n",
    "\n",
    "Things to do:\n",
    "- create connection to server\n",
    "- verify exported data is the same as on SQL server\n",
    "  - execute SQL statement on server to count number of records\n",
    "  - check that both schemas matches\n",
    "  - (optional) statistical analysis (e.g., min-max-avg of numerical columns)\n",
    "- create logical table in Databricks\n",
    "- use Azure Keyvault\n",
    "- we need to create a script, no notebook\n",
    "- the script ideally should run once, and of course not after week 7 (it would break or create issues)\n",
    "  - we really need to make sure we don't delete data\n",
    "- ...\n",
    "\n",
    "Links:\n",
    "\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.jdbc.html\n",
    "- https://discuss.dizzycoding.com/how-to-use-jdbc-source-to-write-and-read-data-in-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "jdbc_username = \"\"\n",
    "jdbc_password = \"\"\n",
    "jdbc_hostname = \"\"\n",
    "jdbc_port = 1433\n",
    "jdbc_database =\"\"\n",
    "jdbc_table=\"\"\n",
    "\n",
    "output_path = \"\"\n",
    "output_csv_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdbc:sqlserver://DSBAZAPP0317.dsb.dk:1433;database=DWDSA;'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};database={jdbc_database};\"\n",
    "jdbc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_properties = {\n",
    "    \"user\": jdbc_username,\n",
    "    \"password\": jdbc_password\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Data\n",
    "mssql_df = spark.read.jdbc(jdbc_url, table=jdbc_table, properties=jdbc_properties)\n",
    "display(mssql_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Data\n",
    "mssql_df.write.format(\"delta\").save(output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- create connection to server\n",
    "- verify exported data is the same as on SQL server\n",
    "  - execute SQL statement on server to count number of records\n",
    "  - check that both schemas matches\n",
    "  - (optional) statistical analysis (e.g., min-max-avg of numerical columns)\n",
    "- create logical table in Databricks\n",
    "- use Azure Keyvault\n",
    "- we need to create a script, no notebook\n",
    "- the script ideally should run once, and of course not after week 7 (it would break or create issues)\n",
    "  - we really need to make sure we don't delete data\n",
    "- ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are some ways to solve the above requirements?\n",
    "\n",
    "Advantages and Disadvantages of using a script and Databricks Job\n",
    "- .\n",
    "- .\n",
    "- .\n",
    "\n",
    "Advantages and Disadvantages of using a notebook and running that\n",
    "- .\n",
    "- .\n",
    "- ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux-passenger_prediction-experiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:34:17) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e3b383cc1ba48e162480037e4a97307cce8077f3b37d4bdafd5e3afe800f509"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
